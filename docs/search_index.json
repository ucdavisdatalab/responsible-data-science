[["index.html", "Responsible Data Science Overview 0.1 Workshop Description 0.2 Introduction 0.3 Learning Objectives 0.4 Expectations", " Responsible Data Science Pamela Reynolds, Sydney Wood, &amp; Michael Livanos 2023-02-15 Overview 0.1 Workshop Description Technical advancements through data science combined with the exponential increase in data has led to research breakthroughs across domains and generated entirely new industries. But, lagging behind this growth is our understanding of the evolving socio-technical landscape and ability to predict the indirect consequences of our work. While laws determine the legal parameters governing data use, data science approaches that are technically legal can still be used unethically and irresponsibly, with disastrous consequences from loss of revenue to human rights violations. Through case studies and interactive sessions, this workshop provides an overview of how to practice responsible data science by incorporating considerations of ethics, equity, and justice. We will discuss FACT (fairness, accuracy, confidentiality, and transparency) based approaches to increasing the integrity of our work in data science. 0.2 Introduction This training is intended for anyone who works with data. It is for researchers, students, and other learners who are interested in becoming better data scientists. The topic of responsible data science (RDS) is too broad for a single workshop, and thus this curriculum is designed as an introduction to help you begin to identify how your priorities and the socio-technical and historical context of your data and methods impact your research. Through discussions of case studies and emerging data science movements, we will describe actionable practices that you can implement to improve your research and development workflows. We encourage you to engage with the many additional readings, course materials, and other resources linked herein and continue the dialogue with others in your classes, research groups, and other communities. 0.3 Learning Objectives At the end of this workshop, learners should begin to be able to: Define what ethics, equity, responsibility, and justice mean for the data sciences. Describe examples of how the development of data science can both contribute to inequities, and be leveraged to address them. Begin to identify the underlying context, goals, and incentives influencing their data-driven research. Assess whether a research project’s data meets FAIR (Findable, Accessible, Interoperable and Reusable) criteria. Use a responsible data science (RDS) framework to evaluate the potential impact(s) of a research project case study. Revise their research design using FACT(Fairness, Accuracy, Confidentiality, and Transparency) principles. Identify where to go to learn more. 0.4 Expectations The focus of this training is on helping researchers take responsibility for promoting equity and justice in their data science practices and applications. This curriculum therefore includes case studies and discussions of irresponsible practices, inequities, and injustices. Many of these adverse effects most severely affect vulnerable, disenfranchised, and/or oppressed populations. Confronting these topics is often disturbing and difficult, and we acknowledge that while uncomfortable these studies are based on real life experiences and should not be dismissed. We invite you to reflect upon and share your own experiences on these topics. Each case study starts with a brief overview and concludes with a summary. If you find a particular case study to be traumatizing, skip to the summary and take a look at the additional resources for alternate case studies and resources on the topic. We expect every attendee to engage in respectful and equitable communication with workshop organizers and fellow attendees. As a UC Davis hosted workshop, we commit to upholding the Principles of Community and will not tolerate behavior that does not align with those values. Harassment can be reported directly to the workshop instructors or to the instructional team account at datalab-training@ucdavis.edu (note: we are all mandatory reporters). Additional resources can be found at the UC Davis Office of Diversity, Equity and Inclusion. "],["background.html", "1 Background 1.1 Defining Data Science 1.2 History in Context", " 1 Background 1.1 Defining Data Science What is data science? The term emerged in the mid-20th century, but it did not achieve broad recognition as a unique discipline until the early 21st century. The Data Science Journal was founded in 2002, and “data scientist” as a job title appeared for the first time in 2005, rocketing to become the “sexiest job” (according to the Harvard Business Review) in 2012. While a data scientist may be commonly described as someone who writes computer code and uses statistics to ask questions, solve problems, and glean insights using data, a universal definition of data science is more nebulous. In his seminal article “50 years of data science,” Donoho (2015) proposed data science as “learning from data, or ‘data analysis’, and scientific studies of data analysis across the spectrum of academic disciplines.” Subsequently, Cao (2017) stated that “data science is a new interdisciplinary field that synthesizes and builds on statistics, informatics, computing, communication, management, and sociology to study data and its environments (including domains and other contextual aspects, such as organizational and social aspects) in order to transform data to insights and decisions by following a data-to-knowledge-to-wisdom thinking and methodology.” Federal funding agencies have adopted these themes, with the National Science Foundation (NSF 2014) defining data science as “the science of planning for, acquisition, management, analysis of, and inference from data” and the National Institute of Health (NIH 2018) defining it as “an interdisciplinary field of inquiry in which quantitative and analytical approaches, processes, and systems are developed and used to extract knowledge and insights from increasingly large and/or complex sets of data.” At the UC Davis DataLab we embrace the commonalities and nuances of these definitions. Here we refer to data science as the integrative, multidisciplinary, translational, and transformative process of extracting knowledge and insights from data. We are committed that these data-driven products should be used for the good. 1.2 History in Context Illustration by Héizel Vázquez. Most descriptions of the history of data science summarize a list of seminal computational and statistical methods published since the mid-1900’s, as in the timeline above. But the practice of data science - abstracting complex phenomena into simpler forms to distill and convey meaning - has been going on since before the agricultural revolution. The development and emergence of data science as a new field has been driven and shaped by our historical and cultural landscapes, similarly to all other domains. Here we will contextualize some modern developments of data science to highlight trends and discuss the oppressive emergence, and emancipatory potential, of the field. 1.2.1 Development of Classic Methods Francis Galton was a 19th century mathematician and is credited with creating statistical analysis tools still commonly used to this day (e.g., standard deviation) and the Central Limit Theorem. He popularized statistical methods in various scientific fields. He was also racist, a product of the eugenics movement. Galton developed these statistical methods in an attempt to demonstrate correlations between one’s own fame and the number of one’s famous relatives. Galton claimed that of the men he examined, significant portions of “eminent” fathers produced “eminent” sons, and that “eminent” sons tended to have “eminent” fathers. He concluded that talent (which he interpreted as intelligence) is hereditary. Galton’s analysis is objectionable in many ways: the arbitrary criteria he used to model “eminence,” a lack of consideration to the social factors of having famous relatives, and his willingness to accept the correlation of a single population as evidence of biological fact, to name a few. Modern data science continues to use these same statistical methods, with their nefarious origin story often left untold in introductory statistics and computer science courses. 1.2.2 Emergence of the Computer Back in Galton’s time (and indeed throughout most of human history), data were scarce. Statisticians often only had access to data they created themselves or with close collaborators. These data were highly localized, accessible only wherever they were recorded, and the capacity to process it limited as mathematicians attempted to analyze it by hand. In this era, individual and societal motives and values had tremendous impact on the accuracy and validity of emerging data-driven methods and their subsequent applications. The rise and commercialization of computers, with their ability to store and share large datasets as well as crunch complex actions, is hailed as a hallmark for democratizing data and making data science more accessible to the general public. But as with Galton, the origins of the technologies enabling this shift were driven by societal needs at a time of war. In March 1940, the invention of the Bombe computer exponentially increased the computational power available. British cryptanalyst Alan Turing created Bombe to decrypt the Nazi Enigma machine, which secured Nazi lines of communication by obscuring them through a complex cipher. The Enigma machines could create 150 trillion unique ciphers using a system of three rotors, and ciphers were changed daily. The Bombe iterated through possible rotor combinations until an expected phrase was found, and programmers would check if this combination revealed the encrypted message. With the creation of the first computer, the dynamic between data availability versus processing power tipped. 1.2.3 Rise of Big Data The ability of compute power to outstrip available data did not last for long. In 1951, magnetic tape was used in computers, allowing data to be stored at a capacity of 128 characters per inch and read at speeds of 12,800 characters per second. By 1954, researchers at IBM and Georgetown University unveiled the 701 Translator. This machine utilized six rules in an attempt to translate Russian to English. It used direct translation of certain words, substitution given limited context, and rearrangement to best fit the English language based on statistical methods. But the goal of this translator wasn’t to revolutionize the literary community and increase access by automating translations. It was designed for use by United States intelligence community against the Soviet Union. It was far cheaper to train an English speaker to use the machine than to learn Russian. Incidentally, the computer was less accurate than trained translators and ultimately intelligence agencies still rely on human translators to this day. But the technological advance However, the attempt revolutionized the data collection and storage capabilities. This type of data storage was scalable, and allowed for the creation of what we would call “big data” today. 1.2.4 The Illusion of Computational Objectivity Corresponding increases in compute technologies to meet the needs of big data cast a veneer of objectivity and fairness onto the field. Systems of oppression are human-created, so machine-driven processes and inferences seemed free from the subjectivity of Galton’s era. But, the data and computational models themselves are not free from human decision. Who decides which sentences are included or excluded from a natural language dataset? Who decided that the vast majority of crash test dummies should represent an ‘average male body’ or, for that matter, what an ‘average male body’ even is? What implications does this have for drivers whose bodies do not conform to those proportions? When crime-prediction datasets are based on historical data on policing, and image datasets contain significantly more white people that people of color, it’s impossible to escape from cultural bias. Whenever data are collected, subjective decisions informed by societal values inevitably impact resulting decision-making models. In our culture, undercurrents of colonialism, white supremacy, and the patriarchy pervade all fields - including data science. Have we really progressed beyond the biases of Galton? "],["dilemma.html", "2 Dilemma 2.1 The Power (and Promises) of Data Science 2.2 The Problem of Data Science 2.3 Good or Evil?", " 2 Dilemma 2.1 The Power (and Promises) of Data Science Data science has contributed to breakthroughs in nearly every discipline, leading to some of the most striking advances and discoveries of the 21st century. Some noteworthy contributions of data science include (Wamba et al., 2015): Creating transparency Enabling experimentation to discover needs Exposing variability and improving performance Segmenting populations to customize actions Replacing/supporting human decision making with automated algorithms Innovating new business models, products and services Within our daily lives, the ability to parse large volumes of data to identify useful information is paramount. To name a few: web search engines optimized to return the most useful links, caller ID warning us not to pick up spam calls, recommender systems helping us find new music, real time traffic updates, speech to text, and instant language translation technologies. These mundane tasks belie revolutionary technological advances enabled by stores of big data that have also been used to elevate living standards and develop life-saving technologies. 2.2 The Problem of Data Science The application of data science has drastically affected cultures and social structures on a rapid scale, sometimes with unintended consequences. For example, web search engines optimized to return the most useful links are built to prioritize popularity without taking veracity into account, which can contribute to the spread of misinformation. Caller ID can create a false sense of security because scammers have learned to make legitimate seeming names and numbers appear. Recommender systems helping us find new music could be limiting the socio-cultural diversity of the artists we listen to. Despite its accolades, data science is also blamed for perpetuating and exacerbating racial bias and harming disenfranchised populations. 2.3 Good or Evil? So, which is it? Is data science the next stage in science’s evolution, necessary for advancing medical, environmental, biological and social science disciplines? Or is data science just a new way for those in power to control wealth, access to information, and influence the masses? Our answer to this dichotomy, and the motivation for this curriculum, is that data science is neither. Data science is not an entity, it does not have agency. Data science is a set of tools and methods that don’t have inherent motivations or goals. Developed by humans living in a particular place and time, data science is both as powerful, imperfect, and biased as the society and data upon which it is based. It is up to us as data scientists, researchers, and practitioners to embrace data ethics, equity, and justice in dictating its responsible development and use. REFLECTION Who benefits from your specific field? What communities experience harm? What are some of the ethical costs of your research? "],["towards-data-ethics-equity-and-justice.html", "3 Towards Data Ethics, Equity and Justice 3.1 Promise And Perils Case Studies 3.2 Discussion", " 3 Towards Data Ethics, Equity and Justice Laws determine the legal parameters governing data use. But something that is legal can still be unethical and irresponsible. Here we imply the definition of ethics as the branch of knowledge establishing the fundamental principles of “right and wrong” behavior. These standards are informative, but not sufficient, for the appropriate management and use of data in today’s age of technology. Data equity provides a lens for considering the multifaceted implications of how data are collected, processed, analyzed, interpreted, and distributed. It is helpful in understanding how data has been historically used to harm marginalized communities and communities of color. Also, data equity allows opportunities for researchers to understand how marginalized communities have been adversely impacted by data misuse through racial bias, hyper surveillance, and stereotyping. Data justice is often tied to data equity as an approach that “redresses the ways of collecting and disseminating data that have invisibilized and harmed historically marginalized communities.” Data justice helps identify the role of data in maintaining systems of power and privilege, knowledge inequity, and harmful decision making. The pursuit of data justice can be a restorative process promoting fairness in how people are made visible and treated because of their digital data. In this workshop we will not debate ethics, equity, or justice. We invite you into this space with the explicit assumption that you seek to produce data-driven research benefiting society. We assume you seek to use data science to generate knowledge while causing as little harm as possible. We aim to give you the intellectual tools to embed values of equity and justice into your own research by focusing on frameworks for responsible use of data and data science methodologies. “Responsible data science promotes best practices that maximize the availability of high quality data while limiting the potential for misuse that could erode fundamental rights and undermine the public trust in digital technologies.”-Digital Society 3.1 Promise And Perils Case Studies Let’s explore responsible data science by unpacking examples of data science enabled automation and accessibility, which are commonly lauded as promoting social good. Read through the assigned case study from the list below. Introduce yourself to your breakout group and discuss the study. To help guide your discussion, consider: What is the real problem the technology was meant to solve? Who determined this was a problem? Who are the stakeholders? Whose voice is being represented? Who is being silenced? Are there any consequences and dilemmas missing from the case study description? What, if any, of the consequences should have been anticipated? Why do you think they were not considered? What strategies, technical or non-technical, should have been included from the beginning to help address potential dilemmas? What can you learn from this study apply to your work? What should we as a research community learn for future automation or accessibility projects? If there is still time, pick and skim another case study. Is there any overlap in your responses? Nominate someone to share out what you discussed. 3.1.1 Automation While automation comes in many forms, perhaps the most salient examples come from changes within the workforce. For example, advances in technology created in part through data science have reduced the danger presented to human workers through replacement or supplementation. 3.1.1.1 Case 1: Robotics in Factories and Mining Summary: Historically, factory and mining jobs have been the most dangerous and unrewarding jobs with relatively low wages and, in many cases, the constant threat of disaster and workplace injury. Robots and other automation technologies were introduced not only to increase productivity, but also to reduce the negative consequences to humans who previously worked those jobs. Here we don’t describe any one employment sector, instead we give an overview of the potential impacts of automation in various workplaces. Good: The rate of workplace death and injury in factory and mining jobs have steadily decreased since robots were first introduced into the manufacturing process in 1980. Additionally, with the design of more ergonomic robots there is the benefit of reducing musculoskeletal disorders, to which factory workers are particularly susceptible. Consequences: Transitioning to robotic labor has had a major socioeconomic impact in the United States. There has been an estimated 50-70% decrease in manufacturing jobs due to automation since 1980. While the incorporation of automation opened new jobs in robot maintenance and manufacturing, many individuals previously employed in factory jobs did not have the skills to fill these new more high-tech jobs, leading to massive unemployment rates, disproportionately affecting older workers. In addition, by shifting the power between management and labor positions, automating the workforce has weakened the power of organized labor, which has been the driving force of labor laws across all employment sectors. 3.1.1.2 Case 2: Automation in Public Safety Summary: In the early 2010s many public safety operations began benefiting from the use of artificial intelligence (AI) robotics and machine learning algorithms. Here we review the impacts of automated labor and decision-making related to “public safety,” broadly defined as emergency responders (i.e., police, military, search and rescue, etc.). Below we discuss the various impacts of the decisions from positive (injury and loss of life prevention) to negative (mental health and human rights impacts). Good: For example, bomb and hazardous material disposal robots began taking the place of safety technicians/specialists, thereby preserving human life. Militarized automated technology, such as flight drones equipped with risk assessment AI, has reduced the number of human pilots in combat zones. More recently, AI robots have been proposed as a supplement or replacement for first responders, such as rescue teams, during natural disasters. Consequences: Some of these robots utilize facial recognition, surveillance AI, and/or are armed with weapons. Decision-making using AI (especially facial recognition) has repeatedly shown racial and ethnic biases. There are major concerns from various human rights organizations, academic institutions and advocacy groups about liability for public safety robot malfunctions, inaccurate or biased decision making by automated software, and privacy concerns about public surveillance and facial recognition. In November of 2022, San Francisco officials voted yes to allow armed robots to be used in “extreme” circumstances. Less than a week later SF regulatory board overturned the decision after public outcry about Killer robots. There is insufficient support for the mental health needs of flight drones’ human pilots, who increasingly suffer from PTSD. Even the more “benign” automated traffic tickets through speedometer and license plate camera disproportionately affect communities of color. In D.C. it was found that black neighborhoods were fined 17 times more often in lower-income black communities compared to white-segregated neighborhoods. Evidence suggests that this disparity is not due to base rate differences in traffic violations. Instead, communities of color had more automated surveiled locations than wealthy parts of the city. In addition, vendors of these technologies have been found to regulate yellow light durations and increase the administrative costs to the city which inflates fine amounts disproportionately directed at marginalized communities. This is only one of the many examples of automation targeting marginalized communities. 3.1.1.3 Case 3: Self-Driving Cars Summary: Just think of the amount of free-time we would have if our cars could drive themselves! Beyond being a very cool and attractive advancement in technology. The implications of automatic vehicles has become a greatly contentious issue due to potential impacts on losses of life, jobs, liability, and marginalized communities. Good: Since 1986 (when the car seat law took effect in all 50 states in the US) through 2021, there have been approximately 40,000 traffic fatalities per year (https://www.nhtsa.gov/data). In theory, self-driven cars should be safer than ones driven by humans. Computer processing speeds are millions of times faster than the average human reaction time, so unexpected collisions could be rare in a future of entirely self-driving cars. Predicted Consequences: Over 2.2 million people are currently employed as drivers in the US. If we switched to fully automated drivers, would there still be a need for these employees? The Canadian government estimates over 1 million jobs in Canada will be lost from self-driving cars. Automation in driving poses the same short-term harm, but long term benefits that automation in the industrial jobs posed in the 1990s and early 2000s. Much of the official discourse focuses on the economic impact of a shift toward automatic vehicles. However, a more important consequence is that self-driving cars rely on the same computer vision technologies that we have already seen to be racially biased – these cars are more likely to kill black pedestrians than white pedestrians (Wilson et al, 2019). Dilemma: Self-driving cars may be the way of the future, but all technological advances take trial and error before being perfected. What would that period of trial and error look like for self-driving cars? Given existing known biases in AI and computer vision technologies, we can expect that marginalized populations will be the most negatively impacted. If the probability of malfunctioning is less than the probability of a crash due to human error, is that good enough to go to market? Who is liable if the AI manages to learn unsafe driving practices from its surroundings? What does liability look like for insurance companies, car manufacturers, car owners? How will this disadvantage individuals who are unable to afford self-driving cars? What is the carbon footprint for something so computationally intensive? These are just a few of the ethics and equity concerns with automated vehicle development that our society has yet to address. 3.1.2 Accessibility Accessibility is a fundamental requirement for equity. Accessibility does not just relate to disability accommodations, although that is incredibly important. It also involves equitable access to resources, information, and opportunities. In addition, accessibility needs are different for different people and in different contexts. While as a society we still have a long way to go in supporting equitable access to various needs for various people, data science has contributed to greater accessibility for a large population of people who have previously had limited access to many quality of life improvements. 3.1.2.1 Case 4: Health Tracking and Precision Medicine Summary: The intersection of biostatistics, data-science, software development and biotechnology has led to an increase in mobile applications (apps) aimed at tracking health metrics and outcomes. These advances have led to a major cultural shift regarding awareness around personal health. Consumer research suggests that over 60% of smartphone users use one or more mobile health (mHealth) apps. There is even evidence that health care workers “prescribe” or recommend specific mHealth apps. In addition most mHealth apps include features that are free to use. However, as these apps are generally not subject to regulation, misuse can lead to negative physical and mental health outcomes, especially in vulnerable populations. Good: On the academic side, the accessibility of vast amounts of non-invasive health data has allowed researchers to develop innovative methods and products. From a public health perspective, many individuals also have a better understanding of general and their personal health trends. Gamification on these apps promotes healthy behaviors like drinking water, exercising, sleep hygiene, mental health interventions (such as mindfulness and coping skills), etc. In addition, health tracking apps can help monitor symptoms and triggers for those with chronic disorders/illnesses like diabetes, Crohn’s disease, mood disorders, and irritable bowel syndrome (IBS). According to the CDC, 9.7% of people who reside in the US were uninsured in 2020. While health apps are not a replacement for actual healthcare, they have the potential to provide valuable services that make substantial differences in health outcomes in underserved populations. Unintended Consequences: Despite the increased usage of smart devices in low socio-economic populations and the advocacy around their potential for combating health inequities, evidence suggests that the full potential of mHealth apps is still only accessible to a small percentage of people underserved by health care systems. In addition, while mHealth apps may be accurate “on average,” they may not be appropriate for a given user’s physiology and over-reliance on mass-produced health apps thus poses safety concerns. Weight-loss apps, especially, have a huge potential for misuse and some apps marketed to help with weight loss have been associated with higher rates of disordered eating. Truly individualized, precision medicine focused mHealth apps are likely to continue to be costly, rare, and widely inaccessible to underserved populations. Dilemma: Most health apps used in the US are not regulated by the Food and Drug Administration (FDA); despite what the apps claim there is no guarantee that their information is accurate. Additionally, any information shared over health apps is not guaranteed to be under HIPAA protection. For example, your identifiable health data can be sold to insurance companies, who can use that information to impact your insurance prices and premiums. Employers could use the data for hiring and career decisions. Health data in these apps is more vulnerable to theft by hackers, and has an increased risk for surveillance. 3.1.2.2 Case 5: Improvements for Underserved Populations Summary: Historically, people with sensory impairments have had limited accessibility to the majority of internet and smart device functionality. Only within the last 5 years has text-to-speech, auto-captioning, and AI-generated photo descriptions become mainstream for online content, with tremendous accessibility increases for those with visual and auditory impairments. Good: Data science has also been used to develop “smart” assistive technologies that allow for sound mixing hearing aids, prosthetic limb control, computer vision (medicine recognition, color recognition, physical writing to speech, assistive sign language), and smart cane navigation, to name a few. Unintended Impact: The availability and accessibility of the assistive technologies themselves is often limited. Medical devices in this space are often expensive and/or not deemed medically necessary (and therefore not covered by most health insurance companies), which further disproportionately disadvantages underserved populations. The resulting loss of economic and social opportunities due to barriers in accessing accessible technologies further reinforces existing inequities. Dilemma: There is a severe lack of research regarding the impact or availability of assistive technologies to their relevant communities within the US. Many devices are developed by private corporations and strong copyright laws prevent these technologies from being made available in other countries or locations. Even federally-funded projects typically generate products that are economically non-viable for a majority of the population they aim to support. 3.2 Discussion These case studies illustrate the multifaceted contributions of data science to modern living. The fact that there are unintended consequences and ethical dilemmas associated with data science advancements seeking to promote social good does not negate its role, but instead serves as a note of caution. As data scientists it is our job to predict and reduce the impact of negative consequences that may emerge from our research and development products. The fact that we still have a long way to go should inspire you to continue to innovate the data science tools, methods, and approaches in your domain. "],["assessment.html", "4 Assessment", " 4 Assessment "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
