[["index.html", "Responsible Data Science Overview 0.1 Workshop Description 0.2 Introduction 0.3 Learning Objectives 0.4 Expectations", " Responsible Data Science Pamela Reynolds Sydney Wood Michael Livanos 2023-02-15 Overview 0.1 Workshop Description Technical advancements through data science combined with the exponential increase in data has led to research breakthroughs across domains and generated entirely new industries. But, lagging behind this growth is our understanding of the evolving socio-technical landscape and ability to predict the indirect consequences of our work. While laws determine the legal parameters governing data use, data science approaches that are technically legal can still be used unethically and irresponsibly, with disastrous consequences from loss of revenue to human rights violations. Through case studies and interactive sessions, this workshop provides an overview of how to practice responsible data science by incorporating considerations of ethics, equity, and justice. We will discuss FACT (fairness, accuracy, confidentiality, and transparency) based approaches to increasing the integrity of our work in data science. 0.2 Introduction This training is intended for anyone who works with data. It is for researchers, students, and other learners who are interested in becoming better data scientists. The topic of responsible data science (RDS) is too broad for a single workshop, and thus this curriculum is designed as an introduction to help you begin to identify how your priorities and the socio-technical and historical context of your data and methods impact your research. Through discussions of case studies and emerging data science movements, we will describe actionable practices that you can implement to improve your research and development workflows. We encourage you to engage with the many additional readings, course materials, and other resources linked herein and continue the dialogue with others in your classes, research groups, and other communities. 0.3 Learning Objectives At the end of this workshop, learners should begin to be able to: Define what ethics, equity, responsibility, and justice mean for the data sciences. Describe examples of how the development of data science can both contribute to inequities, and be leveraged to address them. Begin to identify the underlying context, goals, and incentives influencing their data-driven research. Assess whether a research project’s data meets FAIR (Findable, Accessible, Interoperable and Reusable) criteria. Use a responsible data science (RDS) framework to evaluate the potential impact(s) of a research project case study. Revise their research design using FACT(Fairness, Accuracy, Confidentiality, and Transparency) principles. Identify where to go to learn more. 0.4 Expectations The focus of this training is on helping researchers take responsibility for promoting equity and justice in their data science practices and applications. This curriculum therefore includes case studies and discussions of irresponsible practices, inequities, and injustices. Many of these adverse effects most severely affect vulnerable, disenfranchised, and/or oppressed populations. Confronting these topics is often disturbing and difficult, and we acknowledge that while uncomfortable these studies are based on real life experiences and should not be dismissed. We invite you to reflect upon and share your own experiences on these topics. Each case study starts with a brief overview and concludes with a summary. If you find a particular case study to be traumatizing, skip to the summary and take a look at the additional resources for alternate case studies and resources on the topic. We expect every attendee to engage in respectful and equitable communication with workshop organizers and fellow attendees. As a UC Davis hosted workshop, we commit to upholding the Principles of Community and will not tolerate behavior that does not align with those values. Harassment can be reported directly to the workshop instructors or to the instructional team account at datalab-training@ucdavis.edu (note: we are all mandatory reporters). Additional resources can be found at the UC Davis Office of Diversity, Equity and Inclusion. "],["defining-data-science.html", "1 Defining Data Science", " 1 Defining Data Science What is data science? The term emerged in the mid-20th century, but it did not achieve broad recognition as a unique discipline until the early 21st century. The Data Science Journal was founded in 2002, and “data scientist” as a job title appeared for the first time in 2005, rocketing to become the “sexiest job” (according to the Harvard Business Review) in 2012. While a data scientist may be commonly described as someone who writes computer code and uses statistics to ask questions, solve problems, and glean insights using data, a universal definition of data science is more nebulous. Venn Diagram by Drew Conway In his seminal article “50 years of data science,” Donoho (2015) proposed data science as “learning from data, or ‘data analysis’, and scientific studies of data analysis across the spectrum of academic disciplines.” Subsequently, Cao (2017) stated that “data science is a new interdisciplinary field that synthesizes and builds on statistics, informatics, computing, communication, management, and sociology to study data and its environments (including domains and other contextual aspects, such as organizational and social aspects) in order to transform data to insights and decisions by following a data-to-knowledge-to-wisdom thinking and methodology.” Federal funding agencies have adopted these themes, with the National Science Foundation (NSF 2014) defining data science as “the science of planning for, acquisition, management, analysis of, and inference from data” and the National Institute of Health (NIH 2018) defining it as “an interdisciplinary field of inquiry in which quantitative and analytical approaches, processes, and systems are developed and used to extract knowledge and insights from increasingly large and/or complex sets of data.” At the UC Davis DataLab we embrace the commonalities and nuances of these definitions. Here we refer to data science as the integrative, multidisciplinary, translational, and transformative process of extracting knowledge and insights from data. We are committed that these data-driven products should be used for the good. "],["dilemma.html", "2 Dilemma 2.1 The Power (and Promises) of Data Science 2.2 The Problem of Data Science 2.3 Good or Evil?", " 2 Dilemma 2.1 The Power (and Promises) of Data Science Data science has contributed to breakthroughs in nearly every discipline, leading to some of the most striking advances and discoveries of the 21st century. Some noteworthy contributions of data science include (Wamba et al., 2015): Creating transparency Enabling experimentation to discover needs Exposing variability and improving performance Segmenting populations to customize actions Replacing/supporting human decision making with automated algorithms Innovating new business models, products and services Within our daily lives, the ability to parse large volumes of data to identify useful information is paramount. To name a few: web search engines optimized to return the most useful links, caller ID warning us not to pick up spam calls, recommender systems helping us find new music, real time traffic updates, speech to text, and instant language translation technologies. These mundane tasks belie revolutionary technological advances enabled by stores of big data that have also been used to elevate living standards and develop life-saving technologies. 2.2 The Problem of Data Science The application of data science has drastically affected cultures and social structures on a rapid scale, sometimes with unintended consequences. For example, web search engines optimized to return the most useful links are built to prioritize popularity without taking veracity into account, which can contribute to the spread of misinformation. Caller ID can create a false sense of security because scammers have learned to make legitimate seeming names and numbers appear. Recommender systems helping us find new music could be limiting the socio-cultural diversity of the artists we listen to. Despite its accolades, data science is also blamed for perpetuating and exacerbating racial bias and harming disenfranchised populations. 2.3 Good or Evil? So, which is it? Is data science the next stage in science’s evolution, necessary for advancing medical, environmental, biological and social science disciplines? Or is data science just a new way for those in power to control wealth, access to information, and influence the masses? To approach this question, let’s start at the beginning and learn about the history of data science. "],["history-in-context.html", "3 History in Context 3.1 Development of Classic Methods 3.2 Emergence of the Computer 3.3 Rise of Big Data 3.4 Illusion of Computational Objectivity", " 3 History in Context Illustration by Héizel Vázquez. Most descriptions of the history of data science summarize a list of seminal computational and statistical methods published since the mid-1900’s, as in the timeline above. But the practice of data science - abstracting complex phenomena into simpler forms to distill and convey meaning - has been going on since before the agricultural revolution. The development and emergence of data science as a new field has been driven and shaped by our historical and cultural landscapes, similarly to all other domains. Here we will contextualize some modern developments of data science to highlight trends and discuss the oppressive emergence, and emancipatory potential, of the field. 3.1 Development of Classic Methods Francis Galton was a 19th century mathematician and is credited with creating statistical analysis tools still commonly used to this day (e.g., standard deviation) and the Central Limit Theorem. He popularized statistical methods in various scientific fields. He was also racist, a product of the eugenics movement. Galton developed these statistical methods in an attempt to demonstrate correlations between one’s own fame and the number of one’s famous relatives. Galton claimed that of the men he examined, significant portions of “eminent” fathers produced “eminent” sons, and that “eminent” sons tended to have “eminent” fathers. He concluded that talent (which he interpreted as intelligence) is hereditary. Galton’s analysis is objectionable in many ways: the arbitrary criteria he used to model “eminence,” a lack of consideration to the social factors of having famous relatives, and his willingness to accept the correlation of a single population as evidence of biological fact, to name a few. Modern data science continues to use these same statistical methods, with their nefarious origin story often left untold in introductory statistics and computer science courses. 3.2 Emergence of the Computer Back in Galton’s time (and indeed throughout most of human history), data were scarce. Statisticians often only had access to data they created themselves or with close collaborators. These data were highly localized, accessible only wherever they were recorded, and the capacity to process it limited as mathematicians attempted to analyze it by hand. In this era, individual and societal motives and values had tremendous impact on the accuracy and validity of emerging data-driven methods and their subsequent applications. The rise and commercialization of computers, with their ability to store and share large datasets as well as crunch complex actions, is hailed as a hallmark for democratizing data and making data science more accessible to the general public. But as with Galton, the origins of the technologies enabling this shift were driven by societal needs at a time of war. In March 1940, the invention of the Bombe computer exponentially increased the computational power available. British cryptanalyst Alan Turing created Bombe to decrypt the Nazi Enigma machine, which secured Nazi lines of communication by obscuring them through a complex cipher. The Enigma machines could create 150 trillion unique ciphers using a system of three rotors, and ciphers were changed daily. The Bombe iterated through possible rotor combinations until an expected phrase was found, and programmers would check if this combination revealed the encrypted message. With the creation of the first computer, the dynamic between data availability versus processing power tipped. 3.3 Rise of Big Data The ability of compute power to outstrip available data did not last for long. In 1951, magnetic tape was used in computers, allowing data to be stored at a capacity of 128 characters per inch and read at speeds of 12,800 characters per second. By 1954, researchers at IBM and Georgetown University unveiled the 701 Translator. This machine utilized six rules in an attempt to translate Russian to English. It used direct translation of certain words, substitution given limited context, and rearrangement to best fit the English language based on statistical methods. But the goal of this translator wasn’t to revolutionize the literary community and increase access by automating translations. It was designed for use by United States intelligence community against the Soviet Union. It was far cheaper to train an English speaker to use the machine than to learn Russian. Incidentally, the computer was less accurate than trained translators and ultimately intelligence agencies still rely on human translators to this day. But the technological advance However, the attempt revolutionized the data collection and storage capabilities. This type of data storage was scalable, and allowed for the creation of what we would call “big data” today. 3.4 Illusion of Computational Objectivity Corresponding increases in compute technologies to meet the needs of big data cast a veneer of objectivity and fairness onto the field. Systems of oppression are human-created, so machine-driven processes and inferences seemed free from the subjectivity of Galton’s era. But, the data and computational models themselves are not free from human decision. Who decides which sentences are included or excluded from a natural language dataset? Who decided that the vast majority of crash test dummies should represent an ‘average male body’ or, for that matter, what an ‘average male body’ even is? What implications does this have for drivers whose bodies do not conform to those proportions? When crime-prediction datasets are based on historical data on policing, and image datasets contain significantly more white people that people of color, it’s impossible to escape from cultural bias. Whenever data are collected, subjective decisions informed by societal values inevitably impact resulting decision-making models. In our culture, undercurrents of colonialism, white supremacy, and the patriarchy pervade all fields - including data science. Have we really progressed beyond the biases of Galton? "],["towards-data-ethics-equity-and-justice.html", "4 Towards Data Ethics, Equity and Justice 4.1 Ethics 4.2 Equity 4.3 Justice", " 4 Towards Data Ethics, Equity and Justice Our answer to the dichotomy - is data science good or evil - and the motivation for this curriculum, is that data science is neither. Data science is not an entity, it does not have agency. Data science is a set of tools and methods that don’t have inherent motivations or goals. Developed by humans living in a particular place and time, data science is both as powerful, imperfect, and biased as the society and data upon which it is based. It is up to us as data scientists, researchers, and practitioners to embrace data ethics, equity, and justice in dictating its responsible development and use. REFLECTION Who benefits from your specific field? What communities experience harm? What are some of the ethical costs of your research? As we move to address these issues, let’s start with some definitions. “Responsible data science promotes best practices that maximize the availability of high quality data while limiting the potential for misuse that could erode fundamental rights and undermine the public trust in digital technologies.”-Digital Society 4.1 Ethics Laws determine the legal parameters governing data use. But something that is legal can still be unethical and irresponsible. Here we imply the definition of ethics as the branch of knowledge establishing the fundamental principles of “right and wrong” behavior. These standards are informative, but not sufficient, for the appropriate management and use of data in today’s age of technology. 4.2 Equity When practitioners talk about data ethics, we are often signaling the need for data equity. Equity is the process by which we work “toward fair outcomes for people or groups by treating them in ways that address their unique advantages or barriers” (Hernandez, 2022). Data equity thus provides a lens for considering the multifaceted implications of how data are collected, processed, analyzed, interpreted, and distributed. It is helpful in understanding how data has been historically used to harm marginalized communities and communities of color. Data equity also allows opportunities for researchers to understand how marginalized communities have been adversely impacted by data misuse through racial bias, hyper surveillance, and stereotyping. 4.3 Justice Data justice is often tied to data equity as an approach that “redresses the ways of collecting and disseminating data that have invisibilized and harmed historically marginalized communities.” Data justice helps identify the role of data in maintaining systems of power and privilege, knowledge inequity, and harmful decision making. The pursuit of data justice can be a restorative process promoting fairness in how people are made visible and treated because of their digital data. In this workshop we will not debate ethics, equity, or justice. We invite you into this space with the explicit assumption that you seek to produce data-driven research benefiting society. We assume you seek to use data science to generate knowledge while causing as little harm as possible. We aim to give you the intellectual tools to embed values of equity and justice into your own research by focusing on frameworks for responsible use of data and data science methodologies. "],["promise-and-perils-case-studies.html", "5 Promise And Perils Case Studies 5.1 Automation 5.2 Accessibility 5.3 Discussion", " 5 Promise And Perils Case Studies Let’s explore responsible data science by unpacking examples of data science enabled automation and accessibility, which are commonly lauded as promoting social good. Read through the assigned case study from the list below. Introduce yourself to your breakout group and discuss the study. To help guide your discussion, consider: What is the real problem the technology was meant to solve? Who determined this was a problem? Who are the stakeholders? Whose voice is being represented? Who is being silenced? Are there any consequences and dilemmas missing from the case study description? What, if any, of the consequences should have been anticipated? Why do you think they were not considered? * What strategies, technical or non-technical, should have been included from the beginning to help address potential dilemmas? What can you learn from this study apply to your work? What should we as a research community learn for future automation or accessibility projects? If there is still time, pick and skim another case study. Is there any overlap in your responses? Nominate someone to share out what you discussed. 5.1 Automation While automation comes in many forms, perhaps the most salient examples come from changes within the workforce. For example, advances in technology created in part through data science have reduced the danger presented to human workers through replacement or supplementation. 5.1.1 Case 1: Robotics in Factories and Mining Summary: Historically, factory and mining jobs have been the most dangerous and unrewarding jobs with relatively low wages and, in many cases, the constant threat of disaster and workplace injury. Robots and other automation technologies were introduced not only to increase productivity, but also to reduce the negative consequences to humans who previously worked those jobs. Here we don’t describe any one employment sector, instead we give an overview of the potential impacts of automation in various workplaces. Good: The rate of workplace death and injury in factory and mining jobs have steadily decreased since robots were first introduced into the manufacturing process in 1980. Additionally, with the design of more ergonomic robots there is the benefit of reducing musculoskeletal disorders, to which factory workers are particularly susceptible. Consequences: Transitioning to robotic labor has had a major socioeconomic impact in the United States. There has been an estimated 50-70% decrease in manufacturing jobs due to automation since 1980. While the incorporation of automation opened new jobs in robot maintenance and manufacturing, many individuals previously employed in factory jobs did not have the skills to fill these new more high-tech jobs, leading to massive unemployment rates, disproportionately affecting older workers. In addition, by shifting the power between management and labor positions, automating the workforce has weakened the power of organized labor, which has been the driving force of labor laws across all employment sectors. 5.1.2 Case 2: Automation in Public Safety Summary: In the early 2010s many public safety operations began benefiting from the use of artificial intelligence (AI) robotics and machine learning algorithms. Here we review the impacts of automated labor and decision-making related to “public safety,” broadly defined as emergency responders (i.e., police, military, search and rescue, etc.). Below we discuss the various impacts of the decisions from positive (injury and loss of life prevention) to negative (mental health and human rights impacts). Good: For example, bomb and hazardous material disposal robots began taking the place of safety technicians/specialists, thereby preserving human life. Militarized automated technology, such as flight drones equipped with risk assessment AI, has reduced the number of human pilots in combat zones. More recently, AI robots have been proposed as a supplement or replacement for first responders, such as rescue teams, during natural disasters. Consequences: Some of these robots utilize facial recognition, surveillance AI, and/or are armed with weapons. Decision-making using AI (especially facial recognition) has repeatedly shown racial and ethnic biases. There are major concerns from various human rights organizations, academic institutions and advocacy groups about liability for public safety robot malfunctions, inaccurate or biased decision making by automated software, and privacy concerns about public surveillance and facial recognition. In November of 2022, San Francisco officials voted yes to allow armed robots to be used in “extreme” circumstances. Less than a week later SF regulatory board overturned the decision after public outcry about Killer robots. There is insufficient support for the mental health needs of flight drones’ human pilots, who increasingly suffer from PTSD. Even the more “benign” automated traffic tickets through speedometer and license plate camera disproportionately affect communities of color. In D.C. it was found that black neighborhoods were fined 17 times more often in lower-income black communities compared to white-segregated neighborhoods. Evidence suggests that this disparity is not due to base rate differences in traffic violations. Instead, communities of color had more automated surveiled locations than wealthy parts of the city. In addition, vendors of these technologies have been found to regulate yellow light durations and increase the administrative costs to the city which inflates fine amounts disproportionately directed at marginalized communities. This is only one of the many examples of automation targeting marginalized communities. 5.1.3 Case 3: Self-Driving Cars Summary: Just think of the amount of free-time we would have if our cars could drive themselves! Beyond being a very cool and attractive advancement in technology. The implications of automatic vehicles has become a greatly contentious issue due to potential impacts on losses of life, jobs, liability, and marginalized communities. Good: Since 1986 (when the car seat law took effect in all 50 states in the US) through 2021, there have been approximately 40,000 traffic fatalities per year (https://www.nhtsa.gov/data). In theory, self-driven cars should be safer than ones driven by humans. Computer processing speeds are millions of times faster than the average human reaction time, so unexpected collisions could be rare in a future of entirely self-driving cars. Predicted Consequences: Over 2.2 million people are currently employed as drivers in the US. If we switched to fully automated drivers, would there still be a need for these employees? The Canadian government estimates over 1 million jobs in Canada will be lost from self-driving cars. Automation in driving poses the same short-term harm, but long term benefits that automation in the industrial jobs posed in the 1990s and early 2000s. Much of the official discourse focuses on the economic impact of a shift toward automatic vehicles. However, a more important consequence is that self-driving cars rely on the same computer vision technologies that we have already seen to be racially biased – these cars are more likely to kill black pedestrians than white pedestrians (Wilson et al, 2019). Dilemma: Self-driving cars may be the way of the future, but all technological advances take trial and error before being perfected. What would that period of trial and error look like for self-driving cars? Given existing known biases in AI and computer vision technologies, we can expect that marginalized populations will be the most negatively impacted. If the probability of malfunctioning is less than the probability of a crash due to human error, is that good enough to go to market? Who is liable if the AI manages to learn unsafe driving practices from its surroundings? What does liability look like for insurance companies, car manufacturers, car owners? How will this disadvantage individuals who are unable to afford self-driving cars? What is the carbon footprint for something so computationally intensive? These are just a few of the ethics and equity concerns with automated vehicle development that our society has yet to address. 5.2 Accessibility Accessibility is a fundamental requirement for equity. Accessibility does not just relate to disability accommodations, although that is incredibly important. It also involves equitable access to resources, information, and opportunities. In addition, accessibility needs are different for different people and in different contexts. While as a society we still have a long way to go in supporting equitable access to various needs for various people, data science has contributed to greater accessibility for a large population of people who have previously had limited access to many quality of life improvements. 5.2.0.1 Case 4: Health Tracking and Precision Medicine Summary: The intersection of biostatistics, data-science, software development and biotechnology has led to an increase in mobile applications (apps) aimed at tracking health metrics and outcomes. These advances have led to a major cultural shift regarding awareness around personal health. Consumer research suggests that over 60% of smartphone users use one or more mobile health (mHealth) apps. There is even evidence that health care workers “prescribe” or recommend specific mHealth apps. In addition most mHealth apps include features that are free to use. However, as these apps are generally not subject to regulation, misuse can lead to negative physical and mental health outcomes, especially in vulnerable populations. Good: On the academic side, the accessibility of vast amounts of non-invasive health data has allowed researchers to develop innovative methods and products. From a public health perspective, many individuals also have a better understanding of general and their personal health trends. Gamification on these apps promotes healthy behaviors like drinking water, exercising, sleep hygiene, mental health interventions (such as mindfulness and coping skills), etc. In addition, health tracking apps can help monitor symptoms and triggers for those with chronic disorders/illnesses like diabetes, Crohn’s disease, mood disorders, and irritable bowel syndrome (IBS). According to the CDC, 9.7% of people who reside in the US were uninsured in 2020. While health apps are not a replacement for actual healthcare, they have the potential to provide valuable services that make substantial differences in health outcomes in underserved populations. Unintended Consequences: Despite the increased usage of smart devices in low socio-economic populations and the advocacy around their potential for combating health inequities, evidence suggests that the full potential of mHealth apps is still only accessible to a small percentage of people underserved by health care systems. In addition, while mHealth apps may be accurate “on average,” they may not be appropriate for a given user’s physiology and over-reliance on mass-produced health apps thus poses safety concerns. Weight-loss apps, especially, have a huge potential for misuse and some apps marketed to help with weight loss have been associated with higher rates of disordered eating. Truly individualized, precision medicine focused mHealth apps are likely to continue to be costly, rare, and widely inaccessible to underserved populations. Dilemma: Most health apps used in the US are not regulated by the Food and Drug Administration (FDA); despite what the apps claim there is no guarantee that their information is accurate. Additionally, any information shared over health apps is not guaranteed to be under HIPAA protection. For example, your identifiable health data can be sold to insurance companies, who can use that information to impact your insurance prices and premiums. Employers could use the data for hiring and career decisions. Health data in these apps is more vulnerable to theft by hackers, and has an increased risk for surveillance. 5.2.0.2 Case 5: Improvements for Underserved Populations Summary: Historically, people with sensory impairments have had limited accessibility to the majority of internet and smart device functionality. Only within the last 5 years has text-to-speech, auto-captioning, and AI-generated photo descriptions become mainstream for online content, with tremendous accessibility increases for those with visual and auditory impairments. Good: Data science has also been used to develop “smart” assistive technologies that allow for sound mixing hearing aids, prosthetic limb control, computer vision (medicine recognition, color recognition, physical writing to speech, assistive sign language), and smart cane navigation, to name a few. Unintended Impact: The availability and accessibility of the assistive technologies themselves is often limited. Medical devices in this space are often expensive and/or not deemed medically necessary (and therefore not covered by most health insurance companies), which further disproportionately disadvantages underserved populations. The resulting loss of economic and social opportunities due to barriers in accessing accessible technologies further reinforces existing inequities. Dilemma: There is a severe lack of research regarding the impact or availability of assistive technologies to their relevant communities within the US. Many devices are developed by private corporations and strong copyright laws prevent these technologies from being made available in other countries or locations. Even federally-funded projects typically generate products that are economically non-viable for a majority of the population they aim to support. 5.3 Discussion These case studies illustrate the multifaceted contributions of data science to modern living. The fact that there are unintended consequences and ethical dilemmas associated with data science advancements seeking to promote social good does not negate its role, but instead serves as a note of caution. As data scientists it is our job to predict and reduce the impact of negative consequences that may emerge from our research and development products. The fact that we still have a long way to go should inspire you to continue to innovate the data science tools, methods, and approaches in your domain. "],["the-fact-of-data-science.html", "6 The FACT of Data Science 6.1 Case Study: Carbin App for Pothole Identification 6.2 Fairness The data science pipeline (from data collection and management to 6.3 Accuracy 6.4 Confidentiality 6.5 Transparency", " 6 The FACT of Data Science “Responsible data scientists take steps to make data they depend on FAIR (findable, accessible, interoperable, reusable) while ensuring the FACT (fairness, accuray, confidentiality, transparency) of the algorithms and tools they create… their work must be placed in the context of broader social, legal, ethical aspects.” -Digital Society A prominent advocacy movement for responsible data science practices is FACT. This philosophy for data science describes four core areas where we can take responsibility in our data science practices: Fairness, Accuracy, Confidentiality and Transparency. They are not the only factors that contribute to responsible, equitable and just data science, specifically on their own they do not define what relevant contexts are important in avoiding continued structural inequity. However, it does provide an accessible framework with which this workshop will extend to include consideration of historical, socio-technical and structural inequities. Below we will discuss each in turn along with specific actions that will help you develop more responsible and equitable data science practices. 6.1 Case Study: Carbin App for Pothole Identification Summary: In an attempt to reduce the carbon emissions in highly populated areas, a multi-campus contingent consisting of 6 undergraduate and graduate data scientists and 5 faculty members (MIT, UMass Dartmouth, Harvard and Birzeit University in West Bank Palestine) developed algorithms that use accelerometers, navigation software, and other proprioceptive technologies built into most smart phones to identify road conditions. Bad road conditions substantially increase carbon emissions from vehicles. Data: Originally they had planned to develop the app for iPhones specifically, however, one of the students observed iPhones are much more expensive than Android phones and are used disproportionately by those who live in wealthier areas of both Cambridge, USA and West Bank, Palestine. In fact, iPhones are widely inaccessible to Palestinians, so the team developed the app for both types of phones to accommodate for the SES and location-based biases that would result from using only iPhone data. In addition, they recognized the SES imbalances of who has access to cars and utilized volunteers as well as some public transportation services (buses) to gain data in areas that may not be accessed due to lack of local drivers, or drivers without smartphones. Methods: The team recognized that algorithms are likely to pick up on differences in driving style for individuals, weather conditions, and cultural norms. They use highly sophisticated math derived from extensive beta-testing in order to adequately isolate movement caused by road conditions apart from driver-created turbulence. Communication: The team behind Carbin has clearly put thought and work behind their presentation of the software. They use cross-cultural collaboration, consulted with technology experts in other universities. They have developed an informative and accessible website that outlines the goals and expectations for the project. They also communicate the potential disparate impact surrounding using crowdsourcing alone to collect road condition data, and aim to combine crowdsourcing with strategic deployment of drivers to collect data in disadvantaged areas. 6.2 Fairness The data science pipeline (from data collection and management to the development of computational methods for analysis, communication, etc.) should be free from bias. In this case, bias refers not only to prejudices such as racism, sexism, ableism, etc., but also to uncovering the imbalances and unrepresented components in our data. Fairness means that you should only interpret the data with its specific context in mind. Every dataset reflects the biases of the society in which it was produced. If your project touches on sensitive issues or vulnerable populations, it must from the beginning also include collaborators with in-depth cultural and ethical understanding of the issues and context of the data. For projects that do not explicitly interface with such issues and peoples, it likely does so implicitly. No researcher should assume that their work is without fault to marginalized communities and should always consider the sociocultural contexts in which their work is carried out. 6.2.1 “Correcting for Bias” Fairness algorithms in machine learning attempt to account for data biases in one of three ways. The first is referred to as post-hoc fairness, that is, taking a trained model and rearrange its output to be more equitable. What “more equitable” means is up for debate with over twenty definitions[cite] of fairness used in the literature, but typically implies trying to balance the number of people of different protected status variables (PSVs, eg race, sex, gender, marital status, etc.) in the various decisions the learner could make. For example, if a black box model is in charge of determining riskiness for a loan, a data scientist may take the output of the model for a population and rearrange the output so that people of different races are treated equally (e.g., getting the loan and being denied). Rather than making post-hoc decisions, models can also learn to be more fair by propagating a fairness signal. In the same way that a learner processes data and is trained based on how close it got to perfect accuracy, one can train a network so that it predicts people of different PSV’s similarly. Finally, models can also learn to be fair through an adversarial training process. Adversarial learning is the process of training a model alongside an adversary with opposite goals. For fairness, the adversary will look at the output of the learner and try to determine the PSVs of the datum. The model must not only learn how to make predictions about data correctly, but do so in a way that the adversary cannot determine the PSVs of the data. These styles of fairness can be useful in overcoming very direct and overt biases towards particular marginalized groups, and may be a decent starting point for addressing the larger issues of equity in data science, however this still leaves a lot to be desired. These algorithms tend to only look at one very specific type of discrimination and almost never consider the intersection of different attributes. We can make an algorithm fair with respect to individual protected status, but this gives no guarantees that the model will be fair to combinations of such groups. In practice, fairness algorithms are frequently only applied to binary PSVs, or PSVs are binarized to work with existing algorithms. Data scientists will group people into a binary and assume one group will be privileged, and the other oppressed. This is concerning in its own right as it is a textbook example of erasure, but it also obfuscates potential forms of bias that may exist in a model. Further, fairness algorithms are only able to look at a small piece of the whole. Most fairness datasets used in research only look at a single protected status - some algorithms can only optimize on a single PSV - and at the most only 3 PSV’s will be examined at a time. Fairness algorithms assume that PSVs are immutable characteristics of a datum. For example, certain groups of people have been considered a “race” at different points in United States history, and one group of people may be racialized in one society, but not another. Fairness algorithms cannot take this into account. Finally, fairness algorithms and laws governing them provide an easy “out” to the problem of doing data science for equity. If we use a fairness intervention and demonstrate statistical parity, equalized odds, etc, this provides the false impression that we fixed the problem, when in reality there is always the possibility that an algorithm can be unfair in a specific context. While these approaches are better than nothing, they are based on false assumptions about discrimination and are often incapable of evaluating more complex forms of discrimination. Fairness is an aspect we need to be constantly vigilant about, not a check box to tick before shipping out a product. 6.2.2 Case Study: Crowdsourcing City Governance Summary: Departments of housing and transportation in cities across the U.S. have begun using data-driven technologies to make governance decisions (e.g. building development, road improvements, policing patrols, resource allocation etc.) A vast majority of the data used to inform their decisions are collected through “311” complaint hotlines/online reporting. Crowdsourced data can give real-time snapshots of the conditions in different parts of the city, which can be important for future planning as well as emergency response. Fairness Concerns: Reporting behaviors have disparate cultural and historical impacts are different communities in urban areas. Not only are privileged {people who live in a place} more used to asking for what they need, they are also more used to having those needs met with reverence. Conversely, the historical oppression of BIPOC communities by government entities has led to a culture of self-governance and distrust of authority. These known cultural differences have led to major imbalances in crowdsourced 311 data. Wealthy white neighborhoods are more likely to have a higher frequency of calls regarding a wider range of reported content. Whereas, neighborhoods with objectively a higher rate of infrastructural decay are less likely to be reported through official channels and are not represented in the crowdsourced data. The data-driven decision-making is unlikely to correctly account for the bias in the crowdsourced data further contributing to the inequities in resource allocation and distrust for city governance by underserved populations. 6.3 Accuracy When discussing equity and justice, it is often said impact is more important than intention. However, it is also understood that it is impossible to completely control the impact our actions have on any single individual. This is true for research. Most areas of research responsibility involve factors that are difficult to control as researchers. However, we have the most control of the accuracy or quality of the research we produce. To ensure accuracy of data science research, the methods we employ should be backed by theoretical understanding of the inferences and interpretations that are possible through such methods. In addition, we should be utilizing practices that follow statistical and methodological rigor with respect to the context of the data we are using. The abundance of available data allows for greater exploration into datasets by anyone with access and at least a minimal understanding of statistical software. Many of the spurious correlations that are reported and gain traction in popular “science” blogs/news are the results of irresponsible data science methods used by researchers (academic or industry) without theoretical understanding(or purposeful ignorance) of the statistics underlying the method or the context in which the data was collected. Misinterpretation of model output is also a concern in data science methods. Often our models are very complex, so the interpretation of the results and description of what the model is doing can be difficult to clearly present in non-technical language. However, it is the job of the data scientist to be sure that their audience is deriving an accurate interpretation of the results. The other major concern regarding accuracy of interpretations from results and how they are communicated is what we call “visualization crimes”. Even if data is unbiased and correctly analyzed, the results can be visually presented in misleading ways. We won’t go too deep on this topic as there is a wonderful primer on Data Visualization. However, bad data visualization can lead to base-rate fallacies and a conflation of relative vs. absolute risk/probability, so an increased accountability regarding the use of visualizations is needed to promote responsibility through accuracy in data science. 6.3.1 Case Study: Gender and Mentorship Summary: Authors used publication and citation network data which contains author names, affiliations and publication venues for individual papers as nodes with directed connections between nodes indicating papers cited by each paper. They used this data to derive impact measures, gender using a publicly available gender classifier on the names of authors and academic age (#years from first publication) for each author represented in the dataset. From those derived metrics, the authors created mentor-mentee dyads determined by same institutional affiliation and the difference in academic age at time of publication(see paper for more details). In addition to network data, the authors surveyed the identified “junior” researchers regarding their experiences with their mentors. They then sorted mentees based on how much of a “big-shot” the mentor is (their derived impact) to compare the surveyed mentees’ perceived quality of mentorship. The article uses the combination of survey collected measured and network data to report that “big-shot” mentees result in higher ratings of satisfaction with mentee relationship as well as better mentee impact. They also conclude female mentors are less beneficial to female mentees than male mentors with regard to impact, and that female mentors lessen their gain by mentoring female students compared to male students. They do not report any survey findings on the basis of mentor or mentee gender. Accuracy Concerns: This article was very quickly retracted after its publication due to the mentorship &amp; gender interpretations of the results. However, there are many methodological problems that likely affect the accuracy of their claims. I address a select few in detail below. Gender classification. First and foremost, the gender determination method likely introduced inaccuracy and bias. The classifier used gives a probability value for male or female given a full name and country of origin. It is unclear how the authors determined the country of origin for each author. Given the high immigration rates of academics institutional affiliation would be a poor indicator of author origin. It is assumed that the classifier returns the gender that is most probably given the name, however it was not specified by the researchers. The authors reported an “error-rate” of 7% for gender classification, however it did not specify how it determined that error-rate. In fact, recent studies have shown that the specific gender classifier used in this research is incredibly inaccurate for non-anglo names and did not take any non-binary gender identities into account through classification(Wilson et al, 2022). In addition, not only were non-binary identities left out of the classification process there was not even a mention of the potential presence of non-binary genders in academia. Mentor-mentee pairs. The determination of mentor-mentee relationships allow for extreme amounts of bias and noise. The determination was made based on the academic age at time of publication between authors. On all publications any author with an academic age below 7 was considered a mentee and any author with an academic age over 20 was considered a mentor if they share an affiliation in at least one charred publication. In the age of large scale collaborations, this definition of mentor mentee relationships is likely very inaccurate. Survey data. The response rate to the 2000 junior scientists surveyed was 8%, which is fairly standard for survey research, however the sensitive nature of the question is likely to influence not only how participants respond to survey questions, but also who responds to the survey. The data is likely to be highly unrepresentative of junior researchers overall. In addition, due to the way that mentor-mentee pairs are derived, mentees are likely to have multiple mentors. Based on what was reported the survey did not have respondents specify who their mentor was, so matching survey responses to network derived mentor-mentee pairs and impact is not appropriate. There is no guarantee that the surveyed research is describing the same relationship represented in the network. The results for mentor quality by derived metrics are meaningless. Mentor-mentee gender results. In addition to the bias introduced by the gender classifier, the gender comparisons made by the authors in determination of mentee and mentor success is highly biased. The results were interpreted as ineffectual mentoring by female mentors for female mentees compared to male mentors. However, researchers did not account for a variety of known factors that could account for the finding. For instance the general gender bias in publication (very well documented) can easily explain both the low impact on female mentees and mentors. Of course, female researchers who publish with male researchers (whether they are male mentors or male mentees) will have higher publication/citation rates. In addition, data does not account for retention/graduation rates of female mentees which has been well documented to be lower for male mentors than female mentors. I will point out that the researchers’ results are exploratory (aka not part of their original research question). However, they make a point of interpreting the data not as evidence of gender bias in citations (documented in many other places), instead their interpretation implies that female students are better off with male mentors which is counter to the actual research on the subject. 6.4 Confidentiality In order to engage in data science we need data, and in our digital age there is an abundance of data available. However, not all data is created equally. We’ve already talked about how bias or inaccuracy can result in shoddy data science, the proverbial garbage in → garbage out, but even if data is high quality, there still remains the question of whether responsible data science can be done with data obtained through unethical and inequitable means. 6.4.1 Privacy and Data Stewardship In practice, data is often collected with a lack of regard towards the privacy and well-being of those who it represents. When datasets are published or shared, it becomes difficult to address issues after the fact, as the dataset may already be downloaded to dozens of computers. In 2018, Microsoft computer vision teams announced that they had addressed issues surrounding poor accuracy in a “gender classifier”, a network designed to group individuals into a gender binary given a picture of their face, by diversifying the array of skin tones in the data provided. The team reported that their fairness intervention reduced error rates for “men and women of darker skin” by twenty times, and accuracy for women overall by a factor of eight. Setting aside the issue of enforcing a gender binary and the ethical implications of facial recognition for this purpose, the data itself posed privacy concerns when people found images of themselves on this dataset without previously consenting to be part of this dataset, tracing back to Flickr posts. This invasion of privacy and disregard of consent is not an isolated incident. Researchers at the University of North Carolina scraped YouTube videos for people undergoing hormone replacement therapy to bolster their facial recognition datasets without considering the potential implication this has on those individual’s safety. The lack of data stewardship, or oversight to the collection and propagation of this data, poses serious risks to the communities they ostensibly serve. Many online databases (whether they come from social media sites, computer games, news apps, health trackers, etc.) have online APIs where access can be gained for free, often through a simple application process. Many of us know the controversies surrounding buying and selling data in big tech. From concerns regarding surveillance capitalism to copyright and ownership of our own online contributions as intellectual property, everyone is impacted by these concerns in some way. However, as academics, we assume that the general public has the same or similar understanding of the implicit consent that occurs when engaging with anything online. We can rationalize the usage of social media APIs and scraped datasets because as users ourselves we know that everyone has clicked the “Terms and Agreements” check box when making an account. While legally this is an acceptable rationalization. However, there have been significant movements and calls to action to change the standard internet data ownership and sharing to a system more in line with human subjects research by requiring informed consent. Proponents for this model of user data storage and sharing argue it is the responsibility of the platform to ensure that its users are actively and knowingly opting-into “surveillance” rather than our current system where there might be ways to opt-out as a user. By simplifying the language in “terms and conditions”, prominently displaying the “terms and conditions” and allowing for user flexibility in consenting to each data collection/storage usage are all ways to limit the potential abuse of data buying and selling. 6.4.2 De- and Re-Identification of Data Regardless of the ownership status of our data, science has strict autonomy and privacy protections for individual data. Many big data suppliers remove direct identifiers like names, addresses (IP and physical), social security numbers, etc from data that is shared with data scientists. However, even traditionally “de-identified” data often contains enough personal information that very creative or determined data mining techniques may still be able to tie individual identity to their online data. This re-identification process poses a danger to the individuals by leaving them susceptible to identity theft, doxing, leaking of sensitive personal information, etc. Each of which can have disastrous consequences for individuals. The best way to protect individuals’ data privacy is by only sharing data relevant to the analyses or questions relevant to your project. Minimizing the collection of identity specific information, such gps level location data, searches or purchases related to sensitive/protected information (health, sexual or gender identity, religious affiliations, political identities, racial identities) unless absolutely necessary, and even then with specific data protections in place. 6.4.3 Case Study: New York City Taxi Data Summary: In 2014, New York city released data from 173m individual taxi trips including: pick-up and drop off location, time, date, “anonymized” license number and medallion(taxi ID) number, and other meta-data. The data release was aimed at promoting an eco-themed “hack-a-thon” to crowdsource ideas to improve the ecological impact of NYC taxis services. They anonymized the data as a way that would still allow for connecting taxi trips longitudinally, to allow for analysis of individual driver patterns and used hashing to anonymize both license and medallion numbers. Confidentiality Concerns: Hashing the identifying information allowed the full number to be reverse engineered due to the non-randomized hashed values. In addition, the longitudinal data released allows data scientists to estimate how much each driver makes each day. The combination of time-stamped and location allows a data scientist to infer the home address of drivers. In combination with other publicly available information from the NYC Taxis commission a drivers name can easily be mapped to their driver’s license and taxi medallion which could provide major safety concerns for drivers and passengers. Sensitive passenger information could also be potentially identified from geographical locations of trips starting a drop off location. 6.5 Transparency The push toward transparency in science broadly is often referred to as the Open Science movement. Excitingly, data scientists from many different academic disciplines have been at the forefront of this movement which is not a surprise since it is the products that come from data science practices and research that have allowed for many of the tools that facilitate transparency in science. 6.5.1 Open Access as Freedom of Information Often when we talk about transparency, there is also an implication of accessibility (Open Access). Transparent documentation of data or methods are not worth much if they are not made publicly available. Open access allows for a freedom of information that has led to many amazing developments scientifically, but also more broadly. There is growing evidence the cross disciplinary collaboration (which has been facilitated by data sharing and open access publishing) results increase scientific progress measured by information gain and novelty. In addition, there is evidence that transparency increases the diversity in scientific topics of study and by extension greater accessibility for diverse scientists who are underrepresented in academia. Preprints through open access databases have been a major tool of transparency and accessibility in recent years. Accountability Another important reason that transparency is important is for accountability. According to Merton (1957) science is a self-correcting process. Through review and repeated testing, inaccurate findings will eventually be replaced with finding closer to ground truth. However, for repeated testing (or replication) to happen we must have transparency in the exact methods and procedures used to collect and analyze data. Accountability is important for this self-correction process in science, because we must be able to not only identify inaccurate or irresponsible data science. We must be able to change the practices that resulted in the problem. 6.5.2 Research Outcomes Ownership Previously we have talked about data ownership from the perspective of who the data describes; here we are switching focus to talk about ownership or authorship of research outcomes. In a system which incentivizes priority and productivity as a measure of prestige it is understandable to be worried about scooping (Another scientist publishing a research idea/outcome that was shared by another scientist but wasn’t finished “fast enough”). It often seems counter to academic and financial incentives to share data which can allow other data scientists to make discoveries that, given enough time, could have been made by the original owners of the data. However, by pre-registering your research questions, hypotheses and methods on online repositories like OSF, you can document and copyright the research to protect your intellectual property and mitigate the occurrence of scooping. In addition, blockchain allows for ownership markers in data to protect data from being claimed by people other than those who created or collected the data. It also allows us to easily connect data to comprehensive data biographies such as those discussed in other sections of this workshop. 6.5.3 Case Study: Everyone’s an Epidemiologist xkcd Summary: COVID-19 is undeniably the first (or at least most impactful) modern global pandemic since the development of large scale data science methods. Accessibility of data has allowed epidemiologists to identify infection rates, risk factors, disease symptoms, geographic and racial disparities. They have also used real-world datasets along with mathematical and statistical modeling to develop contact tracing methods and policy recommendations. In addition, transparency in science has allowed for rapid information sharing for COVID-19 researchers that allowed for fast and effective developments of treatments, vaccines and prevention techniques. Transparency Concerns: Unfortunately, transparency can also perpetuate irresponsible data science. This was demonstrated most recently through COVID-19 research. The data and methods used by epidemiologists, who have been trained in the theoretical, practical and ethical nuances of disease transmission models, are now accessible to anyone with advanced statistical/computational knowledge without the same domain specific expertise. In addition, the large amount of funding made available to research related to COVID-19 incentivizes researchers from every domain to start tackling questions related to prevention and treatment of COVID-19. In theory, this seems like it would be a positive. However, disease transmission is much more complicated and nuanced than simple contact tracing. Predictive models must consider biological and ecological factors that surpass what other mathematical or statistical modelers might expect. These incentives led to impacted COVID-19 publication rates for COVID-19 research that led to a slow down of publication of trained epidemiologists. In addition, open access practices such as pre-prints allowed non-experts to disseminate their research with ease regardless of the accuracy of those findings. This led to the massive amounts of wide-spread misinformation regarding COVID-19. "],["assessment.html", "7 Assessment", " 7 Assessment "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
